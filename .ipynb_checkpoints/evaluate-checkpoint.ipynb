{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8b71587-a23c-4d43-b504-a5cb5018aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "585117b8-c1d8-4094-993b-fe2372a7b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models/bmw-gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "300b8a0a-df2f-45bb-a52f-f8325b3e61bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from models/bmw-gpt2...\n",
      "\n",
      "--- Qualitative Results (Text Generation) ---\n",
      "\n",
      "PROMPT: The new BMW i7 offers\n",
      "RESULT: The new BMW i7 offers a range of new features and performance in its compact design, including a 7-speed automatic transmission, automatic traction control, automatic braking and an advanced air conditioning system.\n",
      "\n",
      "BMW i7 is available in the following markets:\n",
      "\n",
      "Germany: €199,495\n",
      "\n",
      "Singapore: €189,995\n",
      "\n",
      "Sydney: €189,795\n",
      "\n",
      "Mexico: €189,995\n",
      "\n",
      "Hong Kong: €189,995\n",
      "\n",
      "Poland\n",
      "\n",
      "PROMPT: BMW focuses on sustainability by\n",
      "RESULT: BMW focuses on sustainability by making its product available to customers in a sustainable way. Our company has been around since 2009, and we continue to innovate and take advantage of the latest technology in our vehicles.\n",
      "\n",
      "With our innovative and reliable design and engineering, our customers can take advantage of the latest technologies to ensure their success.\n",
      "\n",
      "Our vehicles are environmentally friendly.\n",
      "\n",
      "Our customers are always looking for the latest and greatest in the latest technology.\n",
      "\n",
      "Our customer service team has been dedicated\n",
      "\n",
      "PROMPT: The driving performance of the M3 is\n",
      "RESULT: The driving performance of the M3 is that it is an electric car, with electric power steering. The steering is controlled by a four-piston caliper. The steering wheel is fully enclosed by an aluminum frame.\n",
      "The M3 is an electric car, with electric power steering. The steering wheel is fully enclosed by an aluminum frame. The M3 is a prototype, so its exact technology profile remains a mystery.\n",
      "The steering wheel is fully enclosed by an aluminum frame.\n",
      "The steering\n",
      "\n",
      "--- Quantitative Results ---\n",
      "Perplexity on test sentence: 19.57\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "    max_length = model.config.n_positions #The Maximum processable sequence length for GPT-2 is typically 1024\n",
    "    stride = 512 #Step size (how far the sliding window moves to the right each time)\n",
    "    seq_len = encodings.input_ids.size(1) #Total length of text\n",
    "\n",
    "    nlls = []\n",
    "    prev_end_loc = 0\n",
    "    for begin_loc in range(0, seq_len, stride):\n",
    "        # Calculate the end position of the current window (which cannot exceed the total length of the article).\n",
    "        end_loc = min(begin_loc + max_length, seq_len)\n",
    "\n",
    "        # Calculate the length of the “new content” to be predicted.\n",
    "        trg_len = end_loc - prev_end_loc \n",
    "\n",
    "        # Extract the tokens for this segment\n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
    "        \n",
    "        target_ids = input_ids.clone()\n",
    "        # Mark “old content viewed in the previous window” as -100\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=target_ids)\n",
    "            # Negative Log-Likelihood Error\n",
    "            neg_log_likelihood = outputs.loss\n",
    "\n",
    "        nlls.append(neg_log_likelihood)\n",
    "        prev_end_loc = end_loc\n",
    "        if end_loc == seq_len:\n",
    "            break\n",
    "    # Take the average of the errors across all segments and calculate the exponential.\n",
    "    ppl = math.exp(torch.stack(nlls).mean())\n",
    "    return ppl\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    output_sequences = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id, # Tell the model that the zero-padding symbol is the end symbol.\n",
    "        num_return_sequences=1 # How many results should be generated at once? Just one is sufficient here.\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "def main():\n",
    "    print(f\"Loading model from {MODEL_PATH}...\")\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "    except OSError:\n",
    "        print(\"Model not found. Please run train.py first.\")\n",
    "        return\n",
    "\n",
    "    # 1. Initial questions for qualitative evaluation (subjective test)\n",
    "    prompts = [\n",
    "        \"The new BMW i7 offers\",\n",
    "        \"BMW focuses on sustainability by\",\n",
    "        \"The driving performance of the M3 is\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n--- Qualitative Results (Text Generation) ---\")\n",
    "    # Asking questions in turn\n",
    "    for p in prompts:\n",
    "        generated = generate_text(model, tokenizer, p)\n",
    "        print(f\"\\nPROMPT: {p}\")\n",
    "        print(f\"RESULT: {generated}\")\n",
    "\n",
    "    # 2. quantitative Evaluation (Perplexity on a sample string) (objective test)\n",
    "    # Ideally, this is done on the whole test set, but we do a quick check here.\n",
    "    test_text = \"The BMW Group is the world's leading premium manufacturer of automobiles and motorcycles.\"\n",
    "    ppl = calculate_perplexity(model, tokenizer, test_text)\n",
    "    print(f\"\\n--- Quantitative Results ---\")\n",
    "    print(f\"Perplexity on test sentence: {ppl:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ea87fb-d3e3-45ed-873a-d7c5b260c414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
